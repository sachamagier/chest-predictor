{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Rescaling\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-06-04 14:14:19--  https://wagon-public-datasets.s3.amazonaws.com/flowers-dataset.zip\n",
      "Resolving wagon-public-datasets.s3.amazonaws.com (wagon-public-datasets.s3.amazonaws.com)... 52.218.108.210, 52.92.17.17, 52.218.24.130, ...\n",
      "Connecting to wagon-public-datasets.s3.amazonaws.com (wagon-public-datasets.s3.amazonaws.com)|52.218.108.210|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 104983809 (100M) [application/zip]\n",
      "Saving to: ‘../raw_data/flowers-dataset.zip’\n",
      "\n",
      "flowers-dataset.zip 100%[===================>] 100.12M  4.80MB/s    in 32s     \n",
      "\n",
      "2024-06-04 14:14:51 (3.13 MB/s) - ‘../raw_data/flowers-dataset.zip’ saved [104983809/104983809]\n",
      "\n",
      "unzip:  cannot find or open flowers-dataset.zip, flowers-dataset.zip.zip or flowers-dataset.zip.ZIP.\n",
      "flowers-dataset.zip\n"
     ]
    }
   ],
   "source": [
    "################### Here we are loading the flowers dataset, we will have to replace it with the one we need ###################\n",
    "option_1 = True\n",
    "\n",
    "raw_data_path = '../raw_data'\n",
    "\n",
    "if option_1:\n",
    "    !wget https://wagon-public-datasets.s3.amazonaws.com/flowers-dataset.zip -P {raw_data_path}\n",
    "    !unzip flowers-dataset.zip -d {raw_data_path}\n",
    "else:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')\n",
    "\n",
    "!ls {raw_data_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following method to create \n",
    "`X_train, y_train, X_val, y_val, X_test, y_test, num_classes` depending on the `loading_method` you have used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def load_flowers_data(loading_method):\n",
    "    if loading_method == 'colab':\n",
    "        data_path = '/content/drive/My Drive/Deep_learning_data/flowers'\n",
    "    elif loading_method == 'direct':\n",
    "        data_path = 'flowers/'\n",
    "    classes = {'daisy':0, 'dandelion':1, 'rose':2}\n",
    "    imgs = []\n",
    "    labels = []\n",
    "    for (cl, i) in classes.items():\n",
    "        images_path = [elt for elt in os.listdir(os.path.join(data_path, cl)) if elt.find('.jpg')>0]\n",
    "        for img in tqdm(images_path[:300]):\n",
    "            path = os.path.join(data_path, cl, img)\n",
    "            if os.path.exists(path):\n",
    "                image = Image.open(path)\n",
    "                image = image.resize((256, 256))\n",
    "                imgs.append(np.array(image))\n",
    "                labels.append(i)\n",
    "\n",
    "    X = np.array(imgs)\n",
    "    num_classes = len(set(labels))\n",
    "    y = to_categorical(labels, num_classes)\n",
    "     # Finally we shuffle:\n",
    "    p = np.random.permutation(len(X))\n",
    "    X, y = X[p], y[p]\n",
    "\n",
    "    first_split = int(len(imgs) /6.)\n",
    "    second_split = first_split + int(len(imgs) * 0.2)\n",
    "    X_test, X_val, X_train = X[:first_split], X[first_split:second_split], X[second_split:]\n",
    "    y_test, y_val, y_train = y[:first_split], y[first_split:second_split], y[second_split:]\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, num_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:00<00:00, 366.43it/s]\n",
      "100%|██████████| 300/300 [00:00<00:00, 341.56it/s]\n",
      "100%|██████████| 299/299 [00:03<00:00, 91.22it/s] \n"
     ]
    }
   ],
   "source": [
    "# CALL load_flowers_data WITH YOUR PREFERRED METHOD HERE\n",
    "# $DELETE_BEGIN\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, num_classes = load_flowers_data('direct')\n",
    "# $DELETE_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(571, 256, 256, 3) (179, 256, 256, 3) (149, 256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image parameters\n",
    "IMG_HEIGHT = 256 #512\n",
    "IMG_WIDTH = 256 #512\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple model\n",
    "def simple_model():\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Rescaling(1./255, input_shape=(IMG_HEIGHT,IMG_WIDTH,3)))\n",
    "\n",
    "    model.add(Conv2D(16, kernel_size=10, activation='relu'))\n",
    "    model.add(MaxPooling2D(3))\n",
    "\n",
    "    model.add(Conv2D(32, kernel_size=8, activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(3))\n",
    "\n",
    "    model.add(Conv2D(32, kernel_size=6, activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_model = simple_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', patience=5, verbose=0, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling_7 (Rescaling)     (None, 256, 256, 3)       0         \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 247, 247, 16)      4816      \n",
      "                                                                 \n",
      " max_pooling2d_15 (MaxPooli  (None, 82, 82, 16)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_16 (Conv2D)          (None, 75, 75, 32)        32800     \n",
      "                                                                 \n",
      " max_pooling2d_16 (MaxPooli  (None, 25, 25, 32)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          (None, 20, 20, 32)        36896     \n",
      "                                                                 \n",
      " max_pooling2d_17 (MaxPooli  (None, 6, 6, 32)          0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 1152)              0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 100)               115300    \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 3)                 303       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 190115 (742.64 KB)\n",
      "Trainable params: 190115 (742.64 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "s_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "18/18 [==============================] - 9s 431ms/step - loss: 1.0133 - accuracy: 0.4378 - val_loss: 0.8316 - val_accuracy: 0.5419\n",
      "Epoch 2/10\n",
      "18/18 [==============================] - 8s 441ms/step - loss: 0.9202 - accuracy: 0.5236 - val_loss: 0.8488 - val_accuracy: 0.5307\n",
      "Epoch 3/10\n",
      "18/18 [==============================] - 7s 406ms/step - loss: 0.8329 - accuracy: 0.5884 - val_loss: 0.8387 - val_accuracy: 0.5307\n",
      "Epoch 4/10\n",
      "18/18 [==============================] - 7s 412ms/step - loss: 0.7463 - accuracy: 0.6760 - val_loss: 0.6797 - val_accuracy: 0.7039\n",
      "Epoch 5/10\n",
      "18/18 [==============================] - 7s 419ms/step - loss: 0.6734 - accuracy: 0.7461 - val_loss: 0.7194 - val_accuracy: 0.6983\n",
      "Epoch 6/10\n",
      "18/18 [==============================] - 7s 415ms/step - loss: 0.5933 - accuracy: 0.7566 - val_loss: 0.7512 - val_accuracy: 0.6648\n",
      "Epoch 7/10\n",
      "18/18 [==============================] - 8s 420ms/step - loss: 0.5499 - accuracy: 0.7898 - val_loss: 0.6596 - val_accuracy: 0.7263\n",
      "Epoch 8/10\n",
      "18/18 [==============================] - 8s 426ms/step - loss: 0.4871 - accuracy: 0.8144 - val_loss: 0.7280 - val_accuracy: 0.7207\n",
      "Epoch 9/10\n",
      "18/18 [==============================] - 8s 439ms/step - loss: 0.3985 - accuracy: 0.8301 - val_loss: 0.7649 - val_accuracy: 0.7207\n",
      "Epoch 10/10\n",
      "18/18 [==============================] - 8s 442ms/step - loss: 0.3219 - accuracy: 0.8862 - val_loss: 0.8966 - val_accuracy: 0.6983\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = s_model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=10,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 132ms/step - loss: 0.7293 - accuracy: 0.7651\n",
      "6/6 [==============================] - 1s 150ms/step - loss: 0.8966 - accuracy: 0.6983\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = s_model.evaluate(X_test, y_test)\n",
    "val_loss, val_accuracy = s_model.evaluate(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val accuracy: 0.6983240246772766\n",
      "test accuracy: 0.7651006579399109\n"
     ]
    }
   ],
   "source": [
    "print('val accuracy:', val_accuracy)\n",
    "print('test accuracy:', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15487/4256656333.py:6: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  save_model(s_model, os.path.join(models_dir, 'simple_model.h5'))\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import save_model\n",
    "\n",
    "models_dir = '../models'\n",
    "\n",
    "# Ensure that the models directory exists\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# Save the trained model to the models directory\n",
    "save_model(s_model, os.path.join(models_dir, 'simple_model.h5'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chest-predictor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
